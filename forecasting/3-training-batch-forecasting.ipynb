{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate UDTF for Parallel Hyperparameter Tuning in Snowflake\n",
    "\n",
    "This notebook is based on an example described in [Building and deploying a time series forecast with Hex + Snowflake](https://quickstarts.snowflake.com/guide/hex/index.html#0). This entire example higlights how we can use Snowflake to perform parallel hyperparameter to forecast store foot traffic. Please take a look at Chase Romano's article [Parallel Hyperparameter tuning using Snowpark](https://medium.com/snowflake/parallel-hyperparameter-tuning-using-snowpark-53cdec2faf77) for more information.\n",
    "\n",
    "We will begin by establishing our Snowflake connection and Snowpark session. This demo assumes the user has access to the `SYSADMIN` role and a virtual warehouse named `COMPUTE_WH` exists and is available for usage. \n",
    "\n",
    "We're assuming the tables `CALENDAR_INFO`, `HOURLY_TRAFFIC`, and `MODEL_FEATURES` have already been created and populated with data based on the [1-data-ingestion.ipynb](1-data-ingestion.ipynb) and [2-create-feature-table.ipynb](2-create-feature-table.ipynb) notebook in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "import xgboost as xgb\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"database\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"role\": \"SYSADMIN\",\n",
    "    \"warehouse\": \"COMPUTE_WH\",\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_params).create()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the table we're using for our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"MODEL_FEATURES\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our UDTF for parallel hyperparameter tuning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this will be a permanant function, we're going to need to use a stage. We'll use an internal stage named `python_models` where the Python file and its dependencies will be uploaded. \n",
    "\n",
    "We will go ahead and create it first in the event it does not exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"CREATE STAGE IF NOT EXISTS PYTHON_MODELS\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our UDTF. As a friendly reminder, we will need to have accepted the Anaconda terms and conditions to use Anaconda 3rd party packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class forecast:\n",
    "    def __init__(self):\n",
    "        self.date_hour = []\n",
    "        self.from_hour = []\n",
    "        self.COLLEGE_TOWN = []\n",
    "        self.DAYOFWEEK = []\n",
    "        self.MONTH = []\n",
    "        self.YEAR = []\n",
    "        self.HOLIDAY_NAME = []\n",
    "        self.HOURLY_TRAFFIC = []\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        date_hour,\n",
    "        HOURLY_TRAFFIC,\n",
    "        from_hour,\n",
    "        COLLEGE_TOWN,\n",
    "        DAYOFWEEK,\n",
    "        MONTH,\n",
    "        YEAR,\n",
    "        HOLIDAY_NAME,\n",
    "    ):\n",
    "        self.date_hour.append(date_hour)\n",
    "        self.HOURLY_TRAFFIC.append(HOURLY_TRAFFIC)\n",
    "        self.from_hour.append(from_hour)\n",
    "        self.COLLEGE_TOWN.append(COLLEGE_TOWN)\n",
    "        self.DAYOFWEEK.append(DAYOFWEEK)\n",
    "        self.MONTH.append(MONTH)\n",
    "        self.YEAR.append(YEAR)\n",
    "        self.HOLIDAY_NAME.append(HOLIDAY_NAME)\n",
    "\n",
    "    def end_partition(self):\n",
    "        df = pd.DataFrame(\n",
    "            zip(\n",
    "                self.date_hour,\n",
    "                self.HOURLY_TRAFFIC,\n",
    "                self.from_hour,\n",
    "                self.COLLEGE_TOWN,\n",
    "                self.DAYOFWEEK,\n",
    "                self.MONTH,\n",
    "                self.YEAR,\n",
    "                self.HOLIDAY_NAME,\n",
    "            ),\n",
    "            columns=[\n",
    "                \"DATE_HOUR\",\n",
    "                \"HOURLY_TRAFFIC\",\n",
    "                \"HOUR\",\n",
    "                \"COLLEGE_TOWN\",\n",
    "                \"CALENDAR_WEEK_DAY_NBR\",\n",
    "                \"CALENDAR_MTH\",\n",
    "                \"CALENDAR_YEAR\",\n",
    "                \"HOLIDAY_NAME\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # set the time column as our index\n",
    "        df2 = df.set_index(\"DATE_HOUR\")\n",
    "        df2.index = pd.to_datetime(df2.index)\n",
    "\n",
    "        # Converting features to categories for get_dummies\n",
    "        df2[\"CALENDAR_WEEK_DAY_NBR\"] = df2[\"CALENDAR_WEEK_DAY_NBR\"].astype(\"category\")\n",
    "        df2[\"CALENDAR_MTH\"] = df2[\"CALENDAR_MTH\"].astype(\"category\")\n",
    "        df2[\"CALENDAR_YEAR\"] = df2[\"CALENDAR_YEAR\"].astype(\"category\")\n",
    "        df2[\"HOUR\"] = df2[\"HOUR\"].astype(\"category\")\n",
    "        df2[\"HOLIDAY_NAME\"] = df2[\"HOLIDAY_NAME\"].astype(\"category\")\n",
    "        df2[\"COLLEGE_TOWN\"] = df2[\"COLLEGE_TOWN\"].astype(\"category\")\n",
    "\n",
    "        # Use get_dummies for categorical features\n",
    "        final = pd.get_dummies(\n",
    "            data=df2,\n",
    "            columns=[\n",
    "                \"HOLIDAY_NAME\",\n",
    "                \"COLLEGE_TOWN\",\n",
    "                \"CALENDAR_WEEK_DAY_NBR\",\n",
    "                \"CALENDAR_MTH\",\n",
    "                \"CALENDAR_YEAR\",\n",
    "                \"HOUR\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # do the train & forecast split\n",
    "        today = date.today()\n",
    "        yesterday = today - timedelta(days=1)\n",
    "        fourweek = today + timedelta(days=28)\n",
    "        tomorrow = today + timedelta(days=1)\n",
    "\n",
    "        train = final[\n",
    "            (final.index >= pd.to_datetime(\"16-Jun-2018\"))\n",
    "            & (final.index <= pd.to_datetime(yesterday))\n",
    "        ]\n",
    "        forecast = final[\n",
    "            (final.index >= pd.to_datetime(tomorrow))\n",
    "            & (final.index <= pd.to_datetime(fourweek))\n",
    "        ]\n",
    "\n",
    "        X_train = train.drop(\"HOURLY_TRAFFIC\", axis=1)\n",
    "        y_train = train[\"HOURLY_TRAFFIC\"]\n",
    "\n",
    "        X_forecast = forecast.drop(\"HOURLY_TRAFFIC\", axis=1)\n",
    "\n",
    "        # Use XGBoost regressor model\n",
    "        model = xgb.XGBRegressor(n_estimators=200, n_jobs=1)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "        forecast[\"PREDICTION\"] = model.predict(X_forecast)\n",
    "\n",
    "        hours = forecast.index.hour\n",
    "        forecast = pd.concat(\n",
    "            [forecast, pd.DataFrame(hours, index=forecast.index)], axis=1\n",
    "        )\n",
    "        forecast = forecast[[\"DATE_HOUR\", \"PREDICTION\"]]\n",
    "        forecast = forecast.sort_index()\n",
    "        forecast.loc[forecast[\"PREDICTION\"] < 0, \"PREDICTION\"] = 0\n",
    "        forecast[\"DATE\"] = forecast.index.date\n",
    "\n",
    "        # output prediction\n",
    "        for idx, row in forecast.iterrows():\n",
    "            DATE = row[\"DATE\"]\n",
    "            DATE_HOUR = row[\"DATE_HOUR\"]\n",
    "            PREDICTION = row[\"PREDICTION\"]\n",
    "            yield DATE, DATE_HOUR, PREDICTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the UDTF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_forecast = F.udtf(\n",
    "    forecast,\n",
    "    output_schema=T.StructType(\n",
    "        [\n",
    "            T.StructField(\"DATE\", T.DateType()),\n",
    "            T.StructField(\"HOUR_OF_DAY\", T.IntegerType()),\n",
    "            T.StructField(\"HOURLY_FORECAST\", T.FloatType()),\n",
    "        ]\n",
    "    ),\n",
    "    input_types=[\n",
    "        T.TimestampType(),\n",
    "        T.LongType(),\n",
    "        T.LongType(),\n",
    "        T.BooleanType(),\n",
    "        T.LongType(),\n",
    "        T.LongType(),\n",
    "        T.LongType(),\n",
    "        T.StringType(),\n",
    "    ],\n",
    "    name=\"store_forecast\",\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@python_models\",\n",
    "    packages=[\"pandas\", \"xgboost\"],\n",
    "    replace=True,\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the UDTF on Snowpark Optimized WH to run models in parallel and get forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table(\"MODEL_FEATURES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = df.select(\n",
    "    df[\"STORE_ID\"],\n",
    "    (\n",
    "        store_forecast(\n",
    "            df[\"TIME_POINTS\"],\n",
    "            df[\"HOURLY_TRAFFIC\"],\n",
    "            df[\"HOUR\"],\n",
    "            df[\"COLLEGE_TOWN\"],\n",
    "            df[\"CALENDAR_WEEK_DAY_NBR\"],\n",
    "            df[\"CALENDAR_MTH_DAY_NBR\"],\n",
    "            df[\"CALENDAR_YEAR\"],\n",
    "            df[\"HOLIDAY_NAME\"],\n",
    "        ).over(partition_by=df[\"STORE_ID\"])\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the forecast table to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.write.save_as_table(\"FOUR_WEEK_FORECAST\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2054dec0498b5c123877e19b450b0c397daa43145025323e8362f09b588247ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
