{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Snowpark connection and load tables from source data.\n",
    "\n",
    "This notebook is based on example described in [Building and deploying a time series forecast with Hex + Snowflake](https://quickstarts.snowflake.com/guide/hex/index.html#0). This entire example higlights how we can use Snowflake to perform parallel hyperparameter tuning forecasting foot traffic. Please take a look at Chase Romano's article [Parallel Hyperparameter tuning using Snowpark](https://medium.com/snowflake/parallel-hyperparameter-tuning-using-snowpark-53cdec2faf77) for more information.\n",
    "\n",
    "We will begin by establishing our Snowflake connection and Snowpark session. This demo assumes the user has access to the `SYSADMIN` role and a virtual warehouse named `COMPUTE_WH` exists and is available for usage. \n",
    "\n",
    "In the event the database or schema does not exist, the connection will be established without database and schema context, but we will create them in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "connection_params = {\n",
    "    \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"database\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"role\": \"SYSADMIN\",\n",
    "    \"warehouse\": \"COMPUTE_WH\",\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_params).create()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connected earlier using the SYSADMIN role and a virtual warehouse named COMPUTE_WH. Let's create a new database and schema in the event that they do not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {os.environ.get('SNOWFLAKE_DATABASE')}\"\n",
    ").collect()\n",
    "session.sql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS {os.environ.get('SNOWFLAKE_DATABASE')}.{os.environ.get('SNOWFLAKE_SCHEMA')}\"\n",
    ").collect()\n",
    "session.sql(f\"USE DATABASE {os.environ.get('SNOWFLAKE_DATABASE')}\").collect()\n",
    "session.sql(\n",
    "    f\"USE SCHEMA {os.environ.get('SNOWFLAKE_DATABASE')}.{os.environ.get('SNOWFLAKE_SCHEMA')}\"\n",
    ").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to create two Pandas DataFrames based on some CSV files that I have available. These files were generated using a process described in [Building and deploying a time series forecast with Hex + Snowflake](https://quickstarts.snowflake.com/guide/hex/index.html#0). The data is in the `data` directory of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv(\"../data/calendar.csv.gz\")\n",
    "traffic_df = pd.read_csv(\"../data/hourly_traffic.csv.gz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our first Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some information and describe both of these tables to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust those \"object\" types to be more specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df[\"CALENDAR_DATE\"] = pd.to_datetime(calendar_df[\"CALENDAR_DATE\"])\n",
    "calendar_df[\"HOLIDAY_NAME\"] = calendar_df[\"HOLIDAY_NAME\"].astype(\"string\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of the time of this writing, the Snowpark DataFrame from Pandas method converts `datetime64[ns]` to `LongType()` Snowpark types representing [unix time](https://en.wikipedia.org/wiki/Unix_time). We can convert this specific column to make it easier to work with inside of Snowflake. We understand this to be a generic date, so that is what we will convert it to with the `to_date` function. \n",
    "\n",
    "Let's persist this table in Snowflake.\n",
    "\n",
    "I'm using the `overwrite` mode here, but in a typical workflow you would likely want to append to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.create_dataframe(calendar_df).with_column(\n",
    "    \"CALENDAR_DATE\", F.to_date(F.cast(\"CALENDAR_DATE\", T.StringType()))\n",
    ").write.save_as_table(\"CALENDAR_INFO\", mode=\"overwrite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at our table. We can also view the schema to see that the `CALENDAR_DATE` column is now a `DATE` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"CALENDAR_INFO\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our other table for hourly traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORE_ID and COLLEGE_TOWN probably need some adjustments, I don't imagine these columns will need to store numbers up to 9,223,372,036,854,775,807. Let's make them `int16` and `bool` respectively.\n",
    "\n",
    "We will similar conversion as we did with the previous DataFrame. For our time conversion, the `to_datetime` function will still let us use the hour value in the `TIME_POINTS` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df[\"STORE_ID\"] = pd.to_numeric(traffic_df[\"STORE_ID\"], downcast=\"signed\")\n",
    "traffic_df[\"COLLEGE_TOWN\"] = traffic_df[\"COLLEGE_TOWN\"].astype(\"boolean\")\n",
    "traffic_df[\"TIME_POINTS\"] = pd.to_datetime(traffic_df[\"TIME_POINTS\"])\n",
    "traffic_df[\"HOLIDAY_NAME\"] = traffic_df[\"HOLIDAY_NAME\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, less memory. ðŸŽ‰ Our memory usage in this example went from 201.6+ MB to 141.1 MB. \n",
    "\n",
    "Finally, we'll create our Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.create_dataframe(traffic_df).with_column(\n",
    "    \"TIME_POINTS\", F.to_timestamp(F.cast(\"TIME_POINTS\", T.StringType()))\n",
    ").write.save_as_table(\"HOURLY_TRAFFIC\", mode=\"overwrite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"HOURLY_TRAFFIC\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2054dec0498b5c123877e19b450b0c397daa43145025323e8362f09b588247ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
